import os
import torch
from pathlib import Path
from huggingface_hub import snapshot_download, try_to_load_from_cache
from src.pipeline import FluxPipeline
from src.transformer_flux import FluxTransformer2DModel

def check_local_model_exists(model_path_or_id):
    """
    æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨æœ¬åœ°å­˜åœ¨
    
    Args:
        model_path_or_id: æœ¬åœ°è·¯å¾„æˆ–HuggingFaceæ¨¡å‹ID
    
    Returns:
        tuple: (is_local, actual_path)
    """
    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„
    if os.path.exists(model_path_or_id):
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ¨¡å‹æ–‡ä»¶
        required_files = [
            "model_index.json",
            "scheduler/scheduler_config.json", 
            "text_encoder/config.json",
            "text_encoder_2/config.json",
            "transformer/config.json",
            "vae/config.json"
        ]
        
        all_exist = all(os.path.exists(os.path.join(model_path_or_id, file)) 
                       for file in required_files)
        
        if all_exist:
            print(f"âœ… æ‰¾åˆ°å®Œæ•´çš„æœ¬åœ°æ¨¡å‹: {model_path_or_id}")
            return True, model_path_or_id
        else:
            print(f"âŒ æœ¬åœ°è·¯å¾„å­˜åœ¨ä½†æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´: {model_path_or_id}")
            return False, None
    
    # æ£€æŸ¥HuggingFaceç¼“å­˜
    try:
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        cached_path = try_to_load_from_cache(
            repo_id=model_path_or_id,
            filename="model_index.json"
        )
        
        if cached_path is not None:
            # è·å–ç¼“å­˜çš„æ ¹ç›®å½•
            cache_dir = str(Path(cached_path).parent)
            print(f"âœ… æ‰¾åˆ°HuggingFaceç¼“å­˜æ¨¡å‹: {cache_dir}")
            return True, cache_dir
        else:
            print(f"âŒ æ¨¡å‹ä¸åœ¨æœ¬åœ°ç¼“å­˜ä¸­: {model_path_or_id}")
            return False, None
            
    except Exception as e:
        print(f"âŒ æ£€æŸ¥ç¼“å­˜æ—¶å‡ºé”™: {e}")
        return False, None

def load_model_local_only(model_path_or_id, device="cuda", torch_dtype=torch.bfloat16):
    """
    ä»…ä»æœ¬åœ°åŠ è½½æ¨¡å‹ï¼Œä¸è¿›è¡Œè‡ªåŠ¨ä¸‹è½½
    
    Args:
        model_path_or_id: æ¨¡å‹è·¯å¾„æˆ–ID
        device: è®¾å¤‡
        torch_dtype: æ•°æ®ç±»å‹
    
    Returns:
        tuple: (pipeline, transformer) æˆ– (None, None)
    """
    print(f"ğŸ” æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨æœ¬åœ°: {model_path_or_id}")
    
    # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨
    is_local, actual_path = check_local_model_exists(model_path_or_id)
    
    if not is_local:
        print(f"âŒ æ¨¡å‹ä¸åœ¨æœ¬åœ°ï¼Œè¯·å…ˆæ‰‹åŠ¨ä¸‹è½½æ¨¡å‹!")
        print(f"ğŸ’¡ ä¸‹è½½æç¤º:")
        print(f"   æ–¹æ³•1: ä½¿ç”¨ huggingface-cli download {model_path_or_id}")
        print(f"   æ–¹æ³•2: ä½¿ç”¨ä»¥ä¸‹Pythonä»£ç ä¸‹è½½:")
        print(f"   from huggingface_hub import snapshot_download")
        print(f"   snapshot_download(repo_id='{model_path_or_id}', local_dir='./models/{model_path_or_id.replace('/', '-')}')")
        return None, None
    
    try:
        print(f"ğŸ“‚ ä»æœ¬åœ°åŠ è½½æ¨¡å‹: {actual_path}")
        
        # è®¾ç½®æœ¬åœ°æ¨¡å¼ï¼Œé˜²æ­¢æ„å¤–ä¸‹è½½
        os.environ['HF_HUB_OFFLINE'] = '1'
        
        # åŠ è½½pipeline
        print("ğŸ”„ åŠ è½½ FluxPipeline...")
        pipe = FluxPipeline.from_pretrained(
            actual_path, 
            torch_dtype=torch_dtype, 
            device=device,
            local_files_only=True  # å¼ºåˆ¶ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        )
        
        # åŠ è½½transformer
        print("ğŸ”„ åŠ è½½ FluxTransformer2DModel...")
        transformer = FluxTransformer2DModel.from_pretrained(
            actual_path,
            subfolder="transformer",
            torch_dtype=torch_dtype,
            device=device,
            local_files_only=True  # å¼ºåˆ¶ä»…ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
        )
        
        pipe.transformer = transformer
        pipe.to(device)
        
        # æ¢å¤åœ¨çº¿æ¨¡å¼
        if 'HF_HUB_OFFLINE' in os.environ:
            del os.environ['HF_HUB_OFFLINE']
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        return pipe, transformer
        
    except Exception as e:
        # æ¢å¤åœ¨çº¿æ¨¡å¼
        if 'HF_HUB_OFFLINE' in os.environ:
            del os.environ['HF_HUB_OFFLINE']
        
        print(f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
        return None, None

def download_model_if_needed(model_id, local_dir=None):
    """
    æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
    
    Args:
        model_id: HuggingFaceæ¨¡å‹ID
        local_dir: æœ¬åœ°å­˜å‚¨ç›®å½•
    """
    if local_dir is None:
        local_dir = f"./models/{model_id.replace('/', '-')}"
    
    print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹åˆ°: {local_dir}")
    
    try:
        snapshot_download(
            repo_id=model_id,
            local_dir=local_dir,
            local_dir_use_symlinks=False
        )
        print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {local_dir}")
        return local_dir
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return None

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ä½ çš„æ¨¡å‹è·¯å¾„æˆ–ID
    model_paths = [
        "./models/FLUX.1-dev",  # æœ¬åœ°è·¯å¾„
        "black-forest-labs/FLUX.1-dev",  # HFæ¨¡å‹ID
        "/path/to/your/local/model"  # å¦ä¸€ä¸ªæœ¬åœ°è·¯å¾„
    ]
    
    for model_path in model_paths:
        print(f"\n{'='*50}")
        print(f"æµ‹è¯•è·¯å¾„: {model_path}")
        print(f"{'='*50}")
        
        # å°è¯•åŠ è½½
        pipe, transformer = load_model_local_only(model_path)
        
        if pipe is not None and transformer is not None:
            print("ğŸ‰ å¯ä»¥å¼€å§‹ä½¿ç”¨æ¨¡å‹äº†!")
            # è¿™é‡Œå¯ä»¥ç»§ç»­ä½ çš„ä»£ç ...
            break
        else:
            print("â³ å°è¯•ä¸‹ä¸€ä¸ªè·¯å¾„...")
    
    # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œæä¾›ä¸‹è½½é€‰é¡¹
    if pipe is None:
        print(f"\nâ“ æ˜¯å¦è¦ä¸‹è½½æ¨¡å‹? (y/n)")
        choice = input().lower().strip()
        if choice == 'y':
            model_id = "black-forest-labs/FLUX.1-dev"
            downloaded_path = download_model_if_needed(model_id)
            if downloaded_path:
                pipe, transformer = load_model_local_only(downloaded_path)