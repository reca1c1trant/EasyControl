import os
import json
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import numpy as np
from typing import List, Dict, Tuple, Optional
from tqdm import tqdm
import argparse
from pathlib import Path
import logging
from datetime import datetime
from torchvision.transforms.functional import pad
from torchvision import transforms
 
from src.pipeline import FluxPipeline
from src.transformer_flux import FluxTransformer2DModel
from src.lora_helper import set_single_lora
 
# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LAIONFaceDataset(Dataset):
    """LAIONFaceæ•°æ®é›†åŠ è½½å™¨"""
    def __init__(self, data_root: str, subset_size: Optional[int] = None):
        self.data_root = Path(data_root)
        self.images_dir = self.data_root
        
        # åŠ è½½å›¾ç‰‡è·¯å¾„
        metadata_path = self.data_root / "metadata.json"
        if metadata_path.exists():
            with open(metadata_path, 'r') as f:
                self.image_paths = json.load(f)
        else:
            extensions = ['.jpg', '.jpeg', '.png', '.bmp']
            self.image_paths = []
            for ext in extensions:
                self.image_paths.extend(list(self.images_dir.glob(f"*{ext}")))
                self.image_paths.extend(list(self.images_dir.glob(f"*{ext.upper()}")))
            self.image_paths = [str(p) for p in self.image_paths]
        
        if subset_size and subset_size < len(self.image_paths):
            np.random.seed(42)
            indices = np.random.choice(len(self.image_paths), subset_size, replace=False)
            self.image_paths = [self.image_paths[i] for i in indices]
        
        logger.info(f"Loaded {len(self.image_paths)} images from {data_root}")
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        if not os.path.isabs(image_path):
            image_path = self.images_dir / image_path
        
        try:
            image = Image.open(image_path).convert("RGB")
            return {'image': image, 'image_path': str(image_path), 'index': idx}
        except Exception as e:
            logger.warning(f"Failed to load image {image_path}: {e}")
            dummy_image = Image.new('RGB', (512, 512), color='white')
            return {'image': dummy_image, 'image_path': str(image_path), 'index': idx}

class ModifiedFluxPipeline(FluxPipeline):
    """ä¿®æ”¹åçš„FluxPipelineï¼Œæ”¯æŒtensorè¾“å…¥å’Œtensorè¾“å‡º"""
    
    def preprocess_subject_tensor(self, subject_tensor: torch.Tensor, cond_size: int = 512) -> torch.Tensor:
        """
        ç›´æ¥å¤„ç†å·²ç»é¢„å¤„ç†å¥½çš„subject tensor
        å¤ç”¨åŸæœ‰çš„paddingé€»è¾‘ï¼Œä½†è·³è¿‡image_processor.preprocess
        """
        # subject_tensorå·²ç»æ˜¯[1, 3, H, W]æ ¼å¼
        pad_h = cond_size - subject_tensor.shape[-2]
        pad_w = cond_size - subject_tensor.shape[-1]
        
        subject_tensor = pad(
            subject_tensor,
            padding=(int(pad_w / 2), int(pad_h / 2), int(pad_w / 2), int(pad_h / 2)),
            fill=0
        )
        
        return subject_tensor.to(dtype=torch.float32)
    
    @torch.no_grad()
    def __call__(
            self,
            prompt: str,
            subject_tensors: Optional[List[torch.Tensor]] = None,  # æ–°å¢ï¼šæ”¯æŒtensorè¾“å…¥
            subject_images: Optional[List[Image.Image]] = None,    # ä¿æŒå…¼å®¹æ€§
            height: Optional[int] = None,
            width: Optional[int] = None,
            num_inference_steps: int = 28,
            guidance_scale: float = 3.5,
            num_images_per_prompt: Optional[int] = 1,
            generator: Optional[torch.Generator] = None,
            cond_size: int = 512,
            output_type: str = "pil",
            return_latents: bool = False,  # æ–°å¢ï¼šå¯é€‰æ‹©è¿”å›latentsç”¨äºlossè®¡ç®—
            **kwargs
    ):
        """
        ä¿®æ”¹åçš„è°ƒç”¨æ–¹æ³•ï¼Œæ”¯æŒtensorè¾“å…¥
        """
        height = height or self.default_sample_size * self.vae_scale_factor
        width = width or self.default_sample_size * self.vae_scale_factor
        self.cond_size = cond_size
        
        # å¤„ç†subjectè¾“å…¥ - æ”¯æŒtensoræˆ–PIL
        sub_number = 0
        subject_image = None
        
        if subject_tensors is not None:
            sub_number = len(subject_tensors)
            subject_tensor_list = []
            for subject_tensor in subject_tensors:
                # ç›´æ¥å¤„ç†tensorï¼Œè·³è¿‡PILè½¬æ¢
                processed_tensor = self.preprocess_subject_tensor(subject_tensor, cond_size)
                subject_tensor_list.append(processed_tensor)
            subject_image = torch.concat(subject_tensor_list, dim=-2)
            
        elif subject_images is not None:
            # ä¿æŒåŸæœ‰çš„PILå¤„ç†é€»è¾‘
            sub_number = len(subject_images)
            subject_image_ls = []
            for subject_img in subject_images:
                w, h = subject_img.size[:2]
                scale = self.cond_size / max(h, w)
                new_h, new_w = int(h * scale), int(w * scale)
                subject_tensor = self.image_processor.preprocess(subject_img, height=new_h, width=new_w)
                subject_tensor = subject_tensor.to(dtype=torch.float32)
                processed_tensor = self.preprocess_subject_tensor(subject_tensor, cond_size)
                subject_image_ls.append(processed_tensor)
            subject_image = torch.concat(subject_image_ls, dim=-2)
        
        # ç©ºé—´æ¡ä»¶ï¼ˆå½“å‰ä¸ºç©ºï¼‰
        condition_image = None
        cond_number = 0
        
        # å…¶ä½™å¤„ç†ä¿æŒä¸å˜
        batch_size = 1
        device = self._execution_device
        
        # ç¼–ç prompt
        prompt_embeds, pooled_prompt_embeds, text_ids = self.encode_prompt(
            prompt=prompt,
            prompt_2=prompt,
            device=device,
            num_images_per_prompt=num_images_per_prompt,
            max_sequence_length=512,
        )
        
        # å‡†å¤‡latents
        num_channels_latents = self.transformer.config.in_channels // 4
        cond_latents, latent_image_ids, noise_latents = self.prepare_latents(
            batch_size * num_images_per_prompt,
            num_channels_latents,
            height,
            width,
            prompt_embeds.dtype,
            device,
            generator,
            subject_image,
            condition_image,
            None,
            cond_number,
            sub_number
        )
        
        latents = noise_latents
        
        # å‡†å¤‡timesteps
        sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps)
        from .pipeline import calculate_shift, retrieve_timesteps
        
        image_seq_len = latents.shape[1]
        mu = calculate_shift(
            image_seq_len,
            self.scheduler.config.base_image_seq_len,
            self.scheduler.config.max_image_seq_len,
            self.scheduler.config.base_shift,
            self.scheduler.config.max_shift,
        )
        timesteps, num_inference_steps = retrieve_timesteps(
            self.scheduler,
            num_inference_steps,
            device,
            None,
            sigmas,
            mu=mu,
        )
        
        # guidance
        if self.transformer.config.guidance_embeds:
            guidance = torch.full([1], guidance_scale, device=device, dtype=torch.float32)
            guidance = guidance.expand(latents.shape[0])
        else:
            guidance = None
        
        # æ¸…é™¤å’Œç¼“å­˜æ¡ä»¶
        for name, attn_processor in self.transformer.attn_processors.items():
            attn_processor.bank_kv.clear()
        
        # warmupç¼“å­˜
        start_idx = latents.shape[1] - 32
        warmup_latents = latents[:, start_idx:, :]
        warmup_latent_ids = latent_image_ids[start_idx:, :]
        t = torch.tensor([timesteps[0]], device=device)
        timestep = t.expand(warmup_latents.shape[0]).to(latents.dtype)
        _ = self.transformer(
            hidden_states=warmup_latents,
            cond_hidden_states=cond_latents,
            timestep=timestep / 1000,
            guidance=guidance,
            pooled_projections=pooled_prompt_embeds,
            encoder_hidden_states=prompt_embeds,
            txt_ids=text_ids,
            img_ids=warmup_latent_ids,
            joint_attention_kwargs=None,
            return_dict=False,
        )[0]
        
        # å»å™ªå¾ªç¯
        with self.progress_bar(total=num_inference_steps) as progress_bar:
            for i, t in enumerate(timesteps):
                timestep = t.expand(latents.shape[0]).to(latents.dtype)
                noise_pred = self.transformer(
                    hidden_states=latents,
                    cond_hidden_states=cond_latents,
                    timestep=timestep / 1000,
                    guidance=guidance,
                    pooled_projections=pooled_prompt_embeds,
                    encoder_hidden_states=prompt_embeds,
                    txt_ids=text_ids,
                    img_ids=latent_image_ids,
                    joint_attention_kwargs=None,
                    return_dict=False,
                )[0]
                
                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]
                progress_bar.update()
        
        # è§£ç å’Œåå¤„ç†
        if return_latents:
            # è¿”å›è§£ç åçš„tensorç”¨äºlossè®¡ç®—
            latents = self._unpack_latents(latents, height, width, self.vae_scale_factor)
            latents = (latents / self.vae.config.scaling_factor) + self.vae.config.shift_factor
            decoded_tensor = self.vae.decode(latents.to(dtype=self.vae.dtype), return_dict=False)[0]
            return decoded_tensor
        else:
            # æ­£å¸¸è¿”å›PILå›¾åƒ
            latents = self._unpack_latents(latents, height, width, self.vae_scale_factor)
            latents = (latents / self.vae.config.scaling_factor) + self.vae.config.shift_factor
            image = self.vae.decode(latents.to(dtype=self.vae.dtype), return_dict=False)[0]
            image = self.image_processor.postprocess(image, output_type=output_type)
            return image

class OptimizedTensorSpaceAdversarialGenerator:
    """ä¼˜åŒ–åçš„åŸºäºTensorç©ºé—´çš„å¯¹æŠ—æ ·æœ¬ç”Ÿæˆå™¨"""
    
    def __init__(self, 
                 base_path: str = "/openbayes/input/input0",
                 subject_lora_path: str = "/openbayes/input/input0/subject.safetensors",
                 device: str = "cuda",
                 output_dir: str = "./adversarial_results"):
        
        self.device = device
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.clean_dir = self.output_dir / "clean_images"
        self.adversarial_dir = self.output_dir / "adversarial_images"
        self.logs_dir = self.output_dir / "logs"
        
        for dir_path in [self.clean_dir, self.adversarial_dir, self.logs_dir]:
            dir_path.mkdir(exist_ok=True)
        
        logger.info("Initializing optimized EasyControl pipeline...")
        self._init_pipeline(base_path, subject_lora_path)
        
        # æ”»å‡»prompt
        self.attack_prompt = "A SKS on the beach"
        logger.info(f"Using attack prompt: '{self.attack_prompt}'")
    
    def _init_pipeline(self, base_path: str, subject_lora_path: str):
        """åˆå§‹åŒ–ä¿®æ”¹åçš„pipeline"""
        self.pipe = ModifiedFluxPipeline.from_pretrained(
            base_path, 
            torch_dtype=torch.bfloat16, 
            device=self.device,
            local_files_only=True
        )
        
        transformer = FluxTransformer2DModel.from_pretrained(
            base_path, 
            subfolder="transformer",
            torch_dtype=torch.bfloat16, 
            device=self.device,
            local_files_only=True
        )
        self.pipe.transformer = transformer
        self.pipe.to(self.device)
        
        # åŠ è½½subject control LoRA
        set_single_lora(self.pipe.transformer, subject_lora_path, lora_weights=[1], cond_size=512)
        
        # ç¡®ä¿æ¨¡å‹å‚æ•°éœ€è¦æ¢¯åº¦
        self.pipe.transformer.requires_grad_(True)
        logger.info("Optimized pipeline initialized successfully!")
    
    def clear_cache(self):
        """æ¸…é™¤attention cache"""
        for name, attn_processor in self.pipe.transformer.attn_processors.items():
            if hasattr(attn_processor, 'bank_kv'):
                attn_processor.bank_kv.clear()
    
    def preprocess_to_tensor(self, image: Image.Image, cond_size: int = 512) -> torch.Tensor:
        """
        å°†PILå›¾åƒé¢„å¤„ç†ä¸ºtensor - å¤ç”¨å®˜æ–¹é¢„å¤„ç†é€»è¾‘
        æ‰€æœ‰æ“ä½œéƒ½æ˜¯çº¿æ€§çš„ï¼Œæ»¡è¶³å¯å¾®åˆ†è¦æ±‚
        """
        w, h = image.size
        scale = cond_size / max(h, w)
        new_h, new_w = int(h * scale), int(w * scale)
        
        # ä½¿ç”¨å®˜æ–¹çš„image_processor.preprocess
        tensor = self.pipe.image_processor.preprocess(image, height=new_h, width=new_w)
        tensor = tensor.to(dtype=torch.float32)
        
        return tensor.to(device=self.device)
    
    def tensor_to_pil_official(self, tensor: torch.Tensor) -> Image.Image:
        """ä½¿ç”¨å®˜æ–¹æ–¹æ³•å°†tensorè½¬æ¢ä¸ºPILå›¾ç‰‡"""
        if len(tensor.shape) == 4:
            tensor = tensor.squeeze(0)
        
        # ä½¿ç”¨å®˜æ–¹çš„postprocessæ–¹æ³•
        tensor = tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
        image = self.pipe.image_processor.postprocess(tensor, output_type="pil")[0]
        
        return image
    
    def generate_with_tensor_subject(self, prompt: str, subject_tensor: torch.Tensor,
                                   height: int = 1024, width: int = 1024, 
                                   num_inference_steps: int = 20,
                                   return_latents: bool = False) -> torch.Tensor:
        """
        ä½¿ç”¨tensorä½œä¸ºsubjectè¾“å…¥è¿›è¡Œç”Ÿæˆ
        å…³é”®ï¼šæ•´ä¸ªè¿‡ç¨‹ä¿æŒåœ¨tensorç©ºé—´ï¼Œæ¢¯åº¦è¿ç»­
        """
        result = self.pipe(
            prompt=prompt,
            subject_tensors=[subject_tensor],  # ç›´æ¥ä¼ å…¥tensor
            height=height,
            width=width,
            guidance_scale=3.5,
            num_inference_steps=num_inference_steps,
            generator=torch.Generator("cpu").manual_seed(42),
            cond_size=512,
            return_latents=return_latents,  # æ§åˆ¶è¿”å›ç±»å‹
        )
        
        return result
    
    def compute_mse_loss_optimized(self, clean_decoded: torch.Tensor, 
                                 adversarial_tensor: torch.Tensor) -> torch.Tensor:
        """
        ä¼˜åŒ–ç‰ˆæœ¬ï¼šåªè®¡ç®—adversarialçš„è§£ç ç»“æœï¼Œä½¿ç”¨é¢„è®¡ç®—çš„clean_decoded
        ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šé¿å…é‡å¤è®¡ç®—clean_decoded
        """
        try:
            # åªç”Ÿæˆadversarialå›¾åƒçš„VAEè§£ç ç»“æœ  
            adversarial_decoded = self.generate_with_tensor_subject(
                self.attack_prompt, adversarial_tensor, return_latents=True
            )
            self.clear_cache()
            
            # ç›´æ¥ä½¿ç”¨é¢„è®¡ç®—çš„clean_decodedè®¡ç®—MSE
            mse_loss = F.mse_loss(clean_decoded, adversarial_decoded)
            
            return mse_loss
            
        except Exception as e:
            logger.warning(f"Failed to compute optimized MSE loss: {e}")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
    
    def pgd_attack_tensor_space_optimized(self, 
                                        original_image: Image.Image,
                                        epsilon: float = 8/255,
                                        alpha: float = 2/255,
                                        num_iterations: int = 50,
                                        lambda_reg: float = 0.1) -> Tuple[torch.Tensor, Dict]:
        """
        ä¼˜åŒ–åçš„åŸºäºtensorç©ºé—´çš„PGDæ”»å‡»
        æ ¸å¿ƒä¼˜åŒ–ï¼šé¢„è®¡ç®—clean_decodedï¼Œé¿å…é‡å¤è®¡ç®—
        """
        
        # é¢„å¤„ç†åŸå§‹å›¾åƒä¸ºtensor
        clean_tensor = self.preprocess_to_tensor(original_image, cond_size=512)
        clean_tensor.requires_grad_(False)
        
        # å…³é”®ä¼˜åŒ–ï¼šé¢„è®¡ç®—clean_decoded
        logger.info("Pre-computing clean decoded tensor...")
        with torch.no_grad():
            clean_decoded = self.generate_with_tensor_subject(
                self.attack_prompt, clean_tensor, return_latents=True
            )
            self.clear_cache()
            
            # ç¡®ä¿é¢„è®¡ç®—ç»“æœä¸å‚ä¸æ¢¯åº¦è®¡ç®—ï¼Œä½†ä¿æŒè®¾å¤‡å’Œæ•°æ®ç±»å‹ä¸€è‡´
            clean_decoded = clean_decoded.detach().to(device=self.device, dtype=torch.float32)
            clean_decoded.requires_grad_(False)
        
        logger.info(f"Clean decoded tensor shape: {clean_decoded.shape}, device: {clean_decoded.device}")
        
        # åˆå§‹åŒ–å™ªå£°tensor
        noise_tensor = torch.zeros_like(clean_tensor, requires_grad=True, device=self.device)
        noise_tensor.data = (torch.rand_like(clean_tensor) - 0.5) * 2 * epsilon
        
        attack_info = {
            'loss_history': [],
            'mse_history': [],
            'epsilon': epsilon,
            'alpha': alpha,
            'num_iterations': num_iterations,
            'lambda_reg': lambda_reg,
            'attack_prompt': self.attack_prompt,
            'optimization': 'clean_precomputed'  # æ ‡è®°ä½¿ç”¨äº†ä¼˜åŒ–
        }
        
        logger.info(f"Starting optimized tensor-space PGD attack with {num_iterations} iterations")
        logger.info("Clean tensor pre-computed, expecting ~50% speedup")
        
        for i in range(num_iterations):
            noise_tensor.requires_grad_(True)
            
            # åœ¨tensorç©ºé—´ç›´æ¥ç»„åˆï¼Œåˆ©ç”¨é¢„å¤„ç†çš„çº¿æ€§ç‰¹æ€§
            adversarial_tensor = torch.clamp(clean_tensor + noise_tensor, 0, 1)
            
            # éªŒè¯æ‰°åŠ¨ä¿æŒæƒ…å†µ
            actual_perturbation = torch.abs(adversarial_tensor - clean_tensor).max()
            expected_perturbation = torch.abs(noise_tensor).max()
            
            #å…³é”®ä¼˜åŒ–ï¼šä½¿ç”¨é¢„è®¡ç®—çš„clean_decoded
            mse_loss = self.compute_mse_loss_optimized(clean_decoded, adversarial_tensor)
            
            # è®¡ç®—æ­£åˆ™åŒ–é¡¹
            reg_loss = torch.max(torch.abs(noise_tensor))
            
            # æ€»æŸå¤±ï¼šæœ€å¤§åŒ–MSEï¼Œæœ€å°åŒ–å™ªå£°å¹…åº¦
            total_loss = -mse_loss + lambda_reg * reg_loss
            
            # è®°å½•å†å²
            attack_info['loss_history'].append(total_loss.item())
            attack_info['mse_history'].append(mse_loss.item())
            
            if i % 10 == 0:  # å‡å°‘æ—¥å¿—é¢‘ç‡
                logger.info(f"Iter {i+1}/{num_iterations}: MSE={mse_loss.item():.6f}, "
                           f"Reg={reg_loss.item():.6f}, Total={total_loss.item():.6f}, "
                           f"Perturbation={actual_perturbation.item():.6f}, "
                           f"Expected={expected_perturbation.item():.6f}")
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            
            if noise_tensor.grad is None:
                logger.warning("Gradient is None! Check computation graph.")
                break
                
            # PGDæ›´æ–°
            with torch.no_grad():
                noise_tensor.data = noise_tensor.data + alpha * noise_tensor.grad.sign()
                
                # æŠ•å½±åˆ°epsilonçº¦æŸ
                noise_tensor.data = torch.clamp(noise_tensor.data, -epsilon, epsilon)
                
                # ç¡®ä¿adversarial tensoråœ¨[0,1]èŒƒå›´
                temp_adversarial = clean_tensor + noise_tensor.data
                noise_tensor.data = torch.clamp(temp_adversarial, 0, 1) - clean_tensor
            
            # æ¸…é›¶æ¢¯åº¦
            noise_tensor.grad = None
            
            # å†…å­˜æ¸…ç†
            if i % 10 == 0:
                torch.cuda.empty_cache()
        
        logger.info("Optimized PGD attack completed!")
        return noise_tensor.detach(), attack_info
    
    def process_dataset(self, 
                       dataset: LAIONFaceDataset,
                       batch_size: int = 1,
                       epsilon: float = 8/255,
                       alpha: float = 2/255,
                       num_iterations: int = 50,
                       lambda_reg: float = 0.1,
                       save_frequency: int = 100,
                       resume_from: Optional[int] = None) -> None:
        """å¤„ç†æ•´ä¸ªæ•°æ®é›†"""
        
        def custom_collate_fn(batch):
            return batch 
            
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, 
                              num_workers=0, collate_fn=custom_collate_fn)
        
        total_samples = len(dataset)
        start_idx = resume_from or 0
        success_count = 0
        total_mse_improvement = 0.0
        results_log = []
        
        logger.info(f"Starting optimized tensor-space adversarial generation for {total_samples} images")
        logger.info(f"Parameters: epsilon={epsilon}, alpha={alpha}, iterations={num_iterations}")
        logger.info("ğŸ”¥ Using optimized algorithm with clean pre-computation")
        
        with tqdm(dataloader, desc="Processing images (optimized)") as pbar:
            for batch_idx, batch in enumerate(pbar):
                if batch_idx < start_idx:
                    continue
                
                try:
                    original_image = batch[0]['image']
                    image_path = batch[0]['image_path']
                    image_idx = batch[0]['index']
                    
                    # æ£€æŸ¥å›¾ç‰‡è´¨é‡
                    if original_image.size[0] < 256 or original_image.size[1] < 256:
                        logger.warning(f"Skipping small image {image_path}")
                        continue
                    
                    # ğŸ”¥ æ‰§è¡Œä¼˜åŒ–åçš„tensorç©ºé—´PGDæ”»å‡»
                    noise_tensor, attack_info = self.pgd_attack_tensor_space_optimized(
                        original_image=original_image,
                        epsilon=epsilon,
                        alpha=alpha,
                        num_iterations=num_iterations,
                        lambda_reg=lambda_reg
                    )
                    
                    # ç”Ÿæˆæœ€ç»ˆçš„å¯¹æŠ—æ ·æœ¬
                    clean_tensor = self.preprocess_to_tensor(original_image, cond_size=512)
                    adversarial_tensor = torch.clamp(clean_tensor + noise_tensor, 0, 1)
                    
                    # ä½¿ç”¨å®˜æ–¹æ–¹æ³•è½¬æ¢ä¸ºPILå›¾åƒä¿å­˜
                    adversarial_image = self.tensor_to_pil_official(adversarial_tensor)
                    
                    # è®¡ç®—æœ€ç»ˆMSE
                    final_mse = attack_info['mse_history'][-1] if attack_info['mse_history'] else 0
                    
                    # ä¿å­˜ç»“æœ
                    clean_path = self.clean_dir / f"{image_idx:06d}_clean.png"
                    adversarial_path = self.adversarial_dir / f"{image_idx:06d}_adversarial.png"
                    
                    original_image.save(clean_path)
                    adversarial_image.save(adversarial_path)
                    
                    # è®°å½•ç»“æœ
                    result_entry = {
                        'image_idx': image_idx,
                        'original_path': str(image_path),
                        'clean_path': str(clean_path),
                        'adversarial_path': str(adversarial_path),
                        'final_mse': final_mse,
                        'attack_info': attack_info,
                        'timestamp': datetime.now().isoformat(),
                        'optimization_used': True  # æ ‡è®°ä½¿ç”¨äº†ä¼˜åŒ–ç‰ˆæœ¬
                    }
                    results_log.append(result_entry)
                    
                    # ç»Ÿè®¡
                    if final_mse > 0.01:
                        success_count += 1
                    total_mse_improvement += final_mse
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.set_postfix({
                        'Success': f"{success_count}/{batch_idx+1}",
                        'Avg_MSE': f"{total_mse_improvement/(batch_idx+1):.4f}",
                        'Current_MSE': f"{final_mse:.4f}",
                        'Optimized': 'âœ“'
                    })
                    
                    # å®šæœŸä¿å­˜
                    if (batch_idx + 1) % save_frequency == 0:
                        self._save_progress_log(results_log, batch_idx + 1)
                
                except Exception as e:
                    logger.error(f"Failed to process image {batch_idx}: {e}")
                    continue
                
                # å†…å­˜æ¸…ç†
                if batch_idx % 10 == 0:
                    torch.cuda.empty_cache()
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self._save_final_results(results_log, success_count, total_samples)
        logger.info(f"Optimized generation completed! Success rate: {success_count}/{total_samples}")
        logger.info("ğŸ”¥ Optimization resulted in ~50% speedup compared to original version")
    
    def _save_progress_log(self, results_log: List[Dict], current_idx: int):
        """ä¿å­˜è¿›åº¦æ—¥å¿—"""
        log_path = self.logs_dir / f"optimized_progress_{current_idx}.json"
        with open(log_path, 'w') as f:
            json.dump(results_log, f, indent=2)
    
    def _save_final_results(self, results_log: List[Dict], success_count: int, total_samples: int):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        final_log_path = self.logs_dir / "optimized_final_results.json"
        summary = {
            'total_samples': total_samples,
            'success_count': success_count,
            'success_rate': success_count / total_samples if total_samples > 0 else 0,
            'optimization_info': {
                'clean_precomputed': True,
                'estimated_speedup': '~50%',
                'algorithm_version': 'optimized_v1.0'
            },
            'results': results_log,
            'timestamp': datetime.now().isoformat()
        }
        
        with open(final_log_path, 'w') as f:
            json.dump(summary, f, indent=2)

def main():
    parser = argparse.ArgumentParser(description="Optimized tensor-space adversarial generation for EasyControl")
    
    # æ•°æ®é›†å‚æ•°
    parser.add_argument("--data_root", type=str, required=True,
                       help="Root directory of LAIONFace dataset")
    parser.add_argument("--subset_size", type=int, default=None,
                       help="Use only a subset of the dataset")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--base_model", type=str, default="/openbayes/input/input0",
                       help="Base FLUX model path")
    parser.add_argument("--subject_lora", type=str, default="/openbayes/input/input0/subject.safetensors",
                       help="Subject LoRA model path")
    
    # æ”»å‡»å‚æ•°
    parser.add_argument("--epsilon", type=float, default=8/255,
                       help="Maximum perturbation magnitude")
    parser.add_argument("--alpha", type=float, default=2/255,
                       help="PGD step size")
    parser.add_argument("--num_iterations", type=int, default=50,
                       help="Number of PGD iterations")
    parser.add_argument("--lambda_reg", type=float, default=0.1,
                       help="Regularization coefficient")
    
    # ç³»ç»Ÿå‚æ•°
    parser.add_argument("--output_dir", type=str, default="./optimized_adversarial_results",
                       help="Output directory")
    parser.add_argument("--device", type=str, default="cuda",
                       help="Computing device")
    parser.add_argument("--save_frequency", type=int, default=100,
                       help="Save progress every N samples")
    parser.add_argument("--resume_from", type=int, default=None,
                       help="Resume from specific sample index")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ•°æ®é›†
    logger.info(f"Loading dataset from {args.data_root}")
    dataset = LAIONFaceDataset(args.data_root, args.subset_size)
    
    # åˆ›å»ºä¼˜åŒ–åçš„tensorç©ºé—´ç”Ÿæˆå™¨
    logger.info("Initializing optimized tensor-space adversarial generator")
    generator = OptimizedTensorSpaceAdversarialGenerator(
        base_path=args.base_model,
        subject_lora_path=args.subject_lora,
        device=args.device,
        output_dir=args.output_dir
    )
    
    # å¼€å§‹å¤„ç†
    logger.info("Starting optimized tensor-space adversarial generation")
    logger.info("ğŸ”¥ Expected ~50% speedup due to clean pre-computation optimization")
    generator.process_dataset(
        dataset=dataset,
        epsilon=args.epsilon,
        alpha=args.alpha,
        num_iterations=args.num_iterations,
        lambda_reg=args.lambda_reg,
        save_frequency=args.save_frequency,
        resume_from=args.resume_from
    )
    
    logger.info("Optimized generation completed!")

if __name__ == "__main__":
    main()

"""
ä¼˜åŒ–ç‰ˆæœ¬ä½¿ç”¨å‘½ä»¤:
python adversarial_generator.py \
    --data_root /openbayes/input/input0/sample_faces \
    --base_model /openbayes/input/input0 \
    --subject_lora /openbayes/input/input0/subject.safetensors \
    --epsilon 0.03137 \
    --alpha 0.00784 \
    --num_iterations 50 \
    --lambda_reg 0.1 \
    --output_dir ./optimized_adversarial_results \
    --device cuda \
    

"""