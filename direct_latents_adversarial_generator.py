import os
import sys
import json
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import numpy as np
from typing import List, Dict, Tuple, Optional, Union
from tqdm import tqdm
import argparse
from pathlib import Path
import logging
from datetime import datetime
from torchvision.transforms.functional import pad
from torchvision import transforms
 
# ä¿®å¤å¯¼å…¥è·¯å¾„é—®é¢˜
script_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(script_dir)
sys.path.insert(0, script_dir)
sys.path.insert(0, parent_dir)
 
# å°è¯•å¤šç§å¯¼å…¥è·¯å¾„

from src.pipeline import FluxPipeline
from src.transformer_flux import FluxTransformer2DModel
from src.lora_helper import set_single_lora

 
# è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–å†…å­˜
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ["PYTORCH_NO_CUDA_MEMORY_CACHING"] = "1"
 
# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
 
class LAIONFaceDataset(Dataset):
    """LAIONFaceæ•°æ®é›†åŠ è½½å™¨"""
    def __init__(self, data_root: str, subset_size: Optional[int] = None):
        self.data_root = Path(data_root)
        self.images_dir = self.data_root
        
        # åŠ è½½å›¾ç‰‡è·¯å¾„
        metadata_path = self.data_root / "metadata.json"
        if metadata_path.exists():
            with open(metadata_path, 'r') as f:
                self.image_paths = json.load(f)
        else:
            extensions = ['.jpg', '.jpeg', '.png', '.bmp']
            self.image_paths = []
            for ext in extensions:
                self.image_paths.extend(list(self.images_dir.glob(f"*{ext}")))
                self.image_paths.extend(list(self.images_dir.glob(f"*{ext.upper()}")))
            self.image_paths = [str(p) for p in self.image_paths]
        
        if subset_size and subset_size < len(self.image_paths):
            np.random.seed(42)
            indices = np.random.choice(len(self.image_paths), subset_size, replace=False)
            self.image_paths = [self.image_paths[i] for i in indices]
        
        logger.info(f"Loaded {len(self.image_paths)} images from {data_root}")
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        if not os.path.isabs(image_path):
            image_path = self.images_dir / image_path
        
        try:
            image = Image.open(image_path).convert("RGB")
            return {'image': image, 'image_path': str(image_path), 'index': idx}
        except Exception as e:
            logger.warning(f"Failed to load image {image_path}: {e}")
            dummy_image = Image.new('RGB', (512, 512), color='white')
            return {'image': dummy_image, 'image_path': str(image_path), 'index': idx}
 
class GradientDiagnosticsMixin:
    """æ¢¯åº¦è¯Šæ–­æ··åˆç±»"""
    
    def diagnose_gradient_flow(self, tensor: torch.Tensor, name: str = "tensor") -> bool:
        """è¯Šæ–­æ¢¯åº¦æµåŠ¨æƒ…å†µ"""
        if tensor.grad is None:
            logger.warning(f"{name}: No gradient")
            return False
        
        grad_norm = tensor.grad.norm().item()
        grad_max = tensor.grad.abs().max().item()
        grad_min = tensor.grad.abs().min().item()
        
        logger.debug(f"{name} gradient - norm: {grad_norm:.6f}, max: {grad_max:.6f}, min: {grad_min:.6f}")
        
        if grad_norm < 1e-8:
            logger.warning(f"{name}: Gradient too small (vanishing): {grad_norm:.2e}")
            return False
        elif grad_norm > 1e6:
            logger.warning(f"{name}: Gradient too large (exploding): {grad_norm:.2e}")
            return False
        
        return True
    
    def check_model_gradients(self, model):
        """æ£€æŸ¥æ¨¡å‹å„éƒ¨åˆ†æ˜¯å¦å¯ç”¨æ¢¯åº¦"""
        components = {
            'transformer': model.transformer,
            'vae': model.vae,
            'text_encoder': model.text_encoder if hasattr(model, 'text_encoder') else None,
        }
        
        for name, component in components.items():
            if component is not None and hasattr(component, 'parameters'):
                param_count = sum(1 for p in component.parameters())
                grad_enabled_count = sum(1 for p in component.parameters() if p.requires_grad)
                logger.info(f"{name}: {grad_enabled_count}/{param_count} parameters have gradients enabled")
            else:
                logger.info(f"{name}: No parameters or component is None")
 
    def log_memory_usage(self, step_name: str):
        """è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            free = total - reserved
            logger.info(f"{step_name}: GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB, Free: {free:.2f}GB")
 
class MemoryOptimizedFluxPipeline(FluxPipeline, GradientDiagnosticsMixin):
    """å†…å­˜ä¼˜åŒ–çš„FluxPipeline"""
    
    def preprocess_subject_tensor(self, subject_tensor: torch.Tensor, cond_size: int = 512) -> torch.Tensor:
        """é¢„å¤„ç†subject tensor"""
        pad_h = cond_size - subject_tensor.shape[-2]
        pad_w = cond_size - subject_tensor.shape[-1]
        
        subject_tensor = pad(
            subject_tensor,
            padding=(int(pad_w / 2), int(pad_h / 2), int(pad_w / 2), int(pad_h / 2)),
            fill=0
        )
        
        return subject_tensor.to(dtype=torch.bfloat16)

# ä»pipeline.pyå¤åˆ¶çš„è¾…åŠ©å‡½æ•°
def calculate_shift(
        image_seq_len,
        base_seq_len: int = 256,
        max_seq_len: int = 4096,
        base_shift: float = 0.5,
        max_shift: float = 1.16,
):
    m = (max_shift - base_shift) / (max_seq_len - base_seq_len)
    b = base_shift - m * base_seq_len
    mu = image_seq_len * m + b
    return mu

def retrieve_timesteps(
        scheduler,
        num_inference_steps: Optional[int] = None,
        device: Optional[Union[str, torch.device]] = None,
        timesteps: Optional[List[int]] = None,
        sigmas: Optional[List[float]] = None,
        **kwargs,
):
    import inspect
    if timesteps is not None and sigmas is not None:
        raise ValueError("Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values")
    if timesteps is not None:
        accepts_timesteps = "timesteps" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())
        if not accepts_timesteps:
            raise ValueError(
                f"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom"
                f" timestep schedules. Please check whether you are using the correct scheduler."
            )
        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)
        timesteps = scheduler.timesteps
        num_inference_steps = len(timesteps)
    elif sigmas is not None:
        accept_sigmas = "sigmas" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())
        if not accept_sigmas:
            raise ValueError(
                f"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom"
                f" sigmas schedules. Please check whether you are using the correct scheduler."
            )
        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)
        timesteps = scheduler.timesteps
        num_inference_steps = len(timesteps)
    else:
        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)
        timesteps = scheduler.timesteps
    return timesteps, num_inference_steps

def retrieve_latents(encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = "sample"):
    """è·å–VAEç¼–ç ç»“æœ"""
    if hasattr(encoder_output, "latent_dist") and sample_mode == "sample":
        return encoder_output.latent_dist.sample(generator)
    elif hasattr(encoder_output, "latent_dist") and sample_mode == "argmax":
        return encoder_output.latent_dist.mode()
    elif hasattr(encoder_output, "latents"):
        return encoder_output.latents
    else:
        raise AttributeError("Could not access latents of provided encoder_output")
 
class DirectLatentsAdversarialGenerator(GradientDiagnosticsMixin):
    """ç›´æ¥åœ¨ä¸»è¦latentsä¸ŠåŠ æ‰°åŠ¨çš„å¯¹æŠ—æ ·æœ¬ç”Ÿæˆå™¨"""
    
    def __init__(self, 
                 base_path: str = "/openbayes/input/input0",
                 subject_lora_path: str = "/openbayes/input/input0/subject.safetensors",
                 device: str = "cuda",
                 output_dir: str = "./direct_latents_adversarial_results"):
        
        self.device = torch.device(device)  # ä¿®å¤ï¼šè½¬æ¢ä¸ºtorch.deviceå¯¹è±¡
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.clean_dir = self.output_dir / "clean_images"
        self.adversarial_dir = self.output_dir / "adversarial_images"
        self.logs_dir = self.output_dir / "logs"
        
        for dir_path in [self.clean_dir, self.adversarial_dir, self.logs_dir]:
            dir_path.mkdir(exist_ok=True)
        
        logger.info("Initializing DIRECT-LATENTS adversarial generator...")
        self.log_memory_usage("Before initialization")
        
        self._init_pipeline(base_path, subject_lora_path)
        
        self.attack_prompt = "A SKS on the beach"
        logger.info(f"Using attack prompt: '{self.attack_prompt}'")
        
        self.log_memory_usage("After initialization")
        
        # æµ‹è¯•åŸºç¡€åŠŸèƒ½
        self._test_basic_gradients()
        
    def _init_pipeline(self, base_path: str, subject_lora_path: str):
        """åˆå§‹åŒ–pipeline - åªå¯ç”¨Transformeræ¢¯åº¦"""
        logger.info("Loading base model...")
        self.pipe = MemoryOptimizedFluxPipeline.from_pretrained(
            base_path, 
            torch_dtype=torch.bfloat16, 
            local_files_only=True
        )
        
        logger.info("Loading transformer...")
        transformer = FluxTransformer2DModel.from_pretrained(
            base_path, 
            subfolder="transformer",
            torch_dtype=torch.bfloat16, 
            local_files_only=True
        )
        self.pipe.transformer = transformer
        self.pipe.to(self.device)
        
        logger.info("Loading LoRA...")
        # åŠ è½½subject control LoRA
        set_single_lora(self.pipe.transformer, subject_lora_path, lora_weights=[1], cond_size=512)
        
        # å…³é”®ï¼šåªå¯ç”¨Transformeræ¢¯åº¦ï¼ŒVAEå®Œå…¨frozen
        logger.info("Setting up gradients...")
        self.pipe.transformer.requires_grad_(True)
        self.pipe.vae.requires_grad_(False)  # æ˜¾å¼ç¦ç”¨VAEæ¢¯åº¦
        self.pipe.transformer.gradient_checkpointing = True
        # ç¦ç”¨VAEçš„ä¼˜åŒ–åŠŸèƒ½
        if hasattr(self.pipe.vae, 'disable_slicing'):
            self.pipe.vae.disable_slicing()
        if hasattr(self.pipe.vae, 'disable_tiling'):
            self.pipe.vae.disable_tiling()
        
        logger.info("DIRECT-LATENTS pipeline initialized!")
        logger.info("VAE gradients DISABLED, MSE comparison in denoised latent space")
        logger.info(f"VAE dtype: {self.pipe.vae.dtype}")
        
        # è¯Šæ–­æ¢¯åº¦è®¾ç½®
        self.check_model_gradients(self.pipe)
    
    def encode_image_to_latents(self, image: Image.Image) -> torch.Tensor:
        """å°†å›¾åƒç¼–ç ä¸ºlatentsï¼ˆæ— æ¢¯åº¦ï¼‰"""
        with torch.no_grad():  # VAEç¼–ç æ— æ¢¯åº¦
            # é¢„å¤„ç†å›¾åƒ
            tensor = self.preprocess_to_tensor(image, cond_size=512)
            
            # ä¿®å¤ï¼šç¡®ä¿è¾“å…¥tensorä¸VAEæ¨¡å‹æ•°æ®ç±»å‹ä¸€è‡´
            tensor = tensor.to(dtype=self.pipe.vae.dtype, device=str(self.device))
            
            # ä¿®å¤ï¼šgeneratorè®¾å¤‡ç±»å‹ä¸è®¡ç®—è®¾å¤‡ä¸€è‡´
            generator = torch.Generator(str(self.device)).manual_seed(42)
            latents = self._encode_vae_image(tensor, generator)
            
            return latents.to(device=str(self.device), dtype=torch.bfloat16)
    
    def _encode_vae_image(self, image: torch.Tensor, generator: torch.Generator):
        """ç¼–ç VAEå›¾åƒ"""
        # ä¿®å¤ï¼šç¡®ä¿è¾“å…¥ä¸VAEæ¨¡å‹æ•°æ®ç±»å‹ä¸€è‡´
        image = image.to(dtype=self.pipe.vae.dtype, device=str(self.device))
        
        if isinstance(generator, list):
            image_latents = [
                retrieve_latents(self.pipe.vae.encode(image[i: i + 1]), generator=generator[i])
                for i in range(image.shape[0])
            ]
            image_latents = torch.cat(image_latents, dim=0)
        else:
            image_latents = retrieve_latents(self.pipe.vae.encode(image), generator=generator)
 
        image_latents = (image_latents - self.pipe.vae.config.shift_factor) * self.pipe.vae.config.scaling_factor
        return image_latents
    
    def decode_latents_to_tensor(self, latents: torch.Tensor) -> torch.Tensor:
        """å°†latentsè§£ç ä¸ºå›¾åƒtensorï¼ˆç”¨äºæœ€ç»ˆæ˜¾ç¤ºï¼‰"""
        with torch.no_grad():  # æœ€ç»ˆè§£ç ä¸éœ€è¦æ¢¯åº¦
            # ä¿®å¤ï¼šç¡®ä¿latentsä¸VAEæ¨¡å‹æ•°æ®ç±»å‹ä¸€è‡´
            latents = (latents / self.pipe.vae.config.scaling_factor) + self.pipe.vae.config.shift_factor
            latents = latents.to(dtype=self.pipe.vae.dtype, device=str(self.device))
            decoded_tensor = self.pipe.vae.decode(latents, return_dict=False)[0]
            return decoded_tensor
    
    def preprocess_to_tensor(self, image: Image.Image, cond_size: int = 512) -> torch.Tensor:
        """é¢„å¤„ç†å›¾åƒä¸ºtensor"""
        w, h = image.size
        scale = cond_size / max(h, w)
        new_h, new_w = int(h * scale), int(w * scale)
        
        tensor = self.pipe.image_processor.preprocess(image, height=new_h, width=new_w)
        # ä¿®å¤ï¼šæš‚æ—¶ä¿æŒbfloat16ï¼Œåœ¨ä½¿ç”¨æ—¶å†è½¬æ¢ä¸ºVAEçš„æ•°æ®ç±»å‹
        tensor = tensor.to(dtype=torch.bfloat16)
        
        return tensor.to(device=str(self.device))
    
    def prepare_clean_latents_and_conditions(self, subject_image: Image.Image, 
                                           height: int = 1024, width: int = 1024) -> Tuple[torch.Tensor, torch.Tensor, Dict]:
        """
        æŒ‰ç…§pipeline.pyé€»è¾‘å‡†å¤‡clean latentså’Œæ¡ä»¶ä¿¡æ¯
        è¿”å›ï¼š(clean_main_latents, subject_condition_latents, pipeline_components)
        """
        logger.debug("Preparing clean latents using EasyControl pipeline logic...")
        
        batch_size = 1
        device = self.device  # torch.deviceå¯¹è±¡
        cond_size = 512
        
        with torch.no_grad():
            # 1. ç¼–ç promptï¼ˆæŒ‰ç…§pipeline.pyé€»è¾‘ï¼‰
            prompt_embeds, pooled_prompt_embeds, text_ids = self.pipe.encode_prompt(
                prompt=self.attack_prompt,
                prompt_2=self.attack_prompt,
                device=str(device),  # ä¿®å¤ï¼šencode_promptéœ€è¦å­—ç¬¦ä¸²è®¾å¤‡å
                num_images_per_prompt=1,
                max_sequence_length=512,
            )
            logger.debug(f"Prompt embeds shape: {prompt_embeds.shape}")
            
            # 2. å‡†å¤‡subjectå›¾åƒï¼ˆå®Œå…¨æŒ‰ç…§pipeline.pyçš„é¢„å¤„ç†é€»è¾‘ï¼‰
            w, h = subject_image.size
            scale = cond_size / max(h, w)
            new_h, new_w = int(h * scale), int(w * scale)
            
            subject_image_tensor = self.pipe.image_processor.preprocess(subject_image, height=new_h, width=new_w)
            subject_image_tensor = subject_image_tensor.to(dtype=torch.bfloat16)
            
            # Paddingåˆ°cond_size
            pad_h = cond_size - subject_image_tensor.shape[-2]
            pad_w = cond_size - subject_image_tensor.shape[-1]
            subject_image_tensor = pad(
                subject_image_tensor,
                padding=(int(pad_w / 2), int(pad_h / 2), int(pad_w / 2), int(pad_h / 2)),
                fill=0
            )
            logger.debug(f"Subject image tensor shape: {subject_image_tensor.shape}")
            
            # 3. è°ƒç”¨åŸå§‹pipelineçš„prepare_latentsï¼ˆä½†ä¸ä½¿ç”¨spatial_imagesï¼‰
            self.pipe.cond_size = cond_size
            num_channels_latents = self.pipe.transformer.config.in_channels // 4  # 16
            
            generator = torch.Generator(str(device)).manual_seed(42)  # å›ºå®šç§å­
            
            # ç›´æ¥è°ƒç”¨åŸå§‹pipelineçš„prepare_latentsæ–¹æ³•
            cond_latents, latent_image_ids, clean_main_latents = self.pipe.prepare_latents(
                batch_size,
                num_channels_latents,
                height,
                width,
                prompt_embeds.dtype,
                device,  # ä¿®å¤ï¼šprepare_latentséœ€è¦å­—ç¬¦ä¸²è®¾å¤‡å
                generator,
                subject_image=subject_image_tensor,  # subjectå›¾åƒ
                condition_image=None,  # ä¸ä½¿ç”¨spatialæ¡ä»¶
                latents=None,
                cond_number=0,  # æ²¡æœ‰spatialæ¡ä»¶
                sub_number=1    # ä¸€ä¸ªsubject
            )
            
            logger.debug(f"Prepared latents - main: {clean_main_latents.shape}, cond: {cond_latents.shape}, ids: {latent_image_ids.shape}")
            
            # 4. è¿”å›ç»„ä»¶ä¿¡æ¯
            pipeline_components = {
                'prompt_embeds': prompt_embeds,
                'pooled_prompt_embeds': pooled_prompt_embeds,
                'text_ids': text_ids,
                'latent_image_ids': latent_image_ids,
                'height': height,
                'width': width,
                'num_inference_steps': 5,
                'guidance_scale': 3.5
            }
            
            return clean_main_latents, cond_latents, pipeline_components
    
    def denoise_latents_with_perturbation(self, main_latents: torch.Tensor, 
                                        cond_latents: torch.Tensor,
                                        pipeline_components: Dict,
                                        enable_grad: bool = True) -> torch.Tensor:
        """
        ä½¿ç”¨æ‰°åŠ¨åçš„ä¸»è¦latentsè¿›è¡Œå»å™ªï¼Œè¿”å›å»å™ªåçš„latents
        å®Œå…¨æŒ‰ç…§pipeline.pyçš„å»å™ªé€»è¾‘
        """
        if enable_grad:
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
            torch.use_deterministic_algorithms(True, warn_only=True)
        
        logger.debug(f"Starting denoising with main_latents shape: {main_latents.shape}")
        
        # æå–pipelineç»„ä»¶
        prompt_embeds = pipeline_components['prompt_embeds']
        pooled_prompt_embeds = pipeline_components['pooled_prompt_embeds']
        text_ids = pipeline_components['text_ids']
        latent_image_ids = pipeline_components['latent_image_ids']
        height = pipeline_components['height']
        width = pipeline_components['width']
        num_inference_steps = pipeline_components['num_inference_steps']
        guidance_scale = pipeline_components['guidance_scale']
        
        device = self.device  # torch.deviceå¯¹è±¡
        latents = main_latents.clone()  # å¤åˆ¶ä»¥é¿å…ä¿®æ”¹åŸå§‹tensor
        
        with torch.set_grad_enabled(enable_grad):
            # å‡†å¤‡timestepsï¼ˆæŒ‰ç…§pipeline.pyé€»è¾‘ï¼‰
            sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps)
            image_seq_len = latents.shape[1]
            mu = calculate_shift(
                image_seq_len,
                self.pipe.scheduler.config.base_image_seq_len,
                self.pipe.scheduler.config.max_image_seq_len,
                self.pipe.scheduler.config.base_shift,
                self.pipe.scheduler.config.max_shift,
            )
            timesteps, num_inference_steps = retrieve_timesteps(
                self.pipe.scheduler,
                num_inference_steps,
                str(device),  # ä¿®å¤ï¼šscheduleréœ€è¦å­—ç¬¦ä¸²è®¾å¤‡å
                None,
                sigmas,
                mu=mu,
            )
            
            # guidanceè®¾ç½®ï¼ˆæŒ‰ç…§pipeline.pyé€»è¾‘ï¼‰
            if self.pipe.transformer.config.guidance_embeds:
                guidance = torch.full([1], guidance_scale, device=str(device), dtype=torch.bfloat16)
                guidance = guidance.expand(latents.shape[0])
            else:
                guidance = None
            
            # æ¸…é™¤attention cacheï¼ˆæŒ‰ç…§pipeline.pyé€»è¾‘ï¼‰
            for name, attn_processor in self.pipe.transformer.attn_processors.items():
                if hasattr(attn_processor, 'bank_kv'):
                    attn_processor.bank_kv.clear()
                if hasattr(attn_processor, '_cache'):
                    attn_processor._cache = None
            original_training_mode = self.pipe.transformer.training
            if enable_grad:
                self.pipe.transformer.eval()
            # Warmupç¼“å­˜ï¼ˆæŒ‰ç…§pipeline.pyé€»è¾‘ï¼‰
            start_idx = latents.shape[1] - 32
            warmup_latents = latents[:, start_idx:, :]
            warmup_latent_ids = latent_image_ids[start_idx:, :]
            t = torch.tensor([timesteps[0]], device=str(device))
            timestep = t.expand(warmup_latents.shape[0]).to(latents.dtype)
            
            _ = self.pipe.transformer(
                hidden_states=warmup_latents,
                cond_hidden_states=cond_latents,
                timestep=timestep / 1000,
                guidance=guidance,
                pooled_projections=pooled_prompt_embeds,
                encoder_hidden_states=prompt_embeds,
                txt_ids=text_ids,
                img_ids=warmup_latent_ids,
                joint_attention_kwargs=None,
                return_dict=False,
            )[0]
            
            # å»å™ªå¾ªç¯ï¼ˆæŒ‰ç…§pipeline.pyé€»è¾‘ï¼‰
            try:
                for i, t in enumerate(timesteps):
                    torch.cuda.empty_cache()
                    timestep = t.expand(latents.shape[0]).to(latents.dtype)

                    # transformeræ¨ç†
                    noise_pred = self.pipe.transformer(
                        hidden_states=latents,           # ä¸»è¦latentsï¼ˆåŒ…å«æ‰°åŠ¨ï¼‰
                        cond_hidden_states=cond_latents, # æ¡ä»¶latentsï¼ˆå›ºå®šï¼‰
                        timestep=timestep / 1000,
                        guidance=guidance,
                        pooled_projections=pooled_prompt_embeds,
                        encoder_hidden_states=prompt_embeds,
                        txt_ids=text_ids,
                        img_ids=latent_image_ids,
                        joint_attention_kwargs=None,
                        return_dict=False,
                    )[0]

                    # è°ƒåº¦å™¨æ›´æ–°
                    latents = self.pipe.scheduler.step(noise_pred, t, latents, return_dict=False)[0]

                    if i % 5 == 0:
                        logger.debug(f"Denoising step {i+1}/{num_inference_steps}")
            finally:
                # ğŸ”¥ æ¢å¤åŸå§‹çŠ¶æ€
                self.pipe.transformer.train(original_training_mode)
                if enable_grad:
                    torch.backends.cudnn.deterministic = False
                    torch.backends.cudnn.benchmark = True
                    torch.use_deterministic_algorithms(False)

            
            logger.debug(f"Denoising completed, final latents shape: {latents.shape}")
            return latents
    
    def pgd_attack_direct_latents(self, 
                                original_image: Image.Image,
                                epsilon: float = 0.1,
                                alpha: float = 0.02,
                                num_iterations: int = 50,
                                lambda_reg: float = 0.1) -> Tuple[torch.Tensor, Dict]:
        """
        ç›´æ¥åœ¨ä¸»è¦latentsä¸Šè¿›è¡ŒPGDæ”»å‡»
        ç®—æ³•ï¼š
        1. è®°å½•clean_main_latentså’Œclean_denoised_latents  
        2. åœ¨iterationä¸­ç›´æ¥å¯¹main_latentsåŠ å™ªå£°
        3. åœ¨denoised_latentsæ¯”è¾ƒMSE
        4. æ¢¯åº¦æ›´æ–°deltaå™ªå£°
        """
        
        # ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡clean latentså’Œæ¡ä»¶
        logger.info("Preparing clean latents and conditions...")
        self.log_memory_usage("Before clean preparation")
        
        clean_main_latents, subject_condition_latents, pipeline_components = self.prepare_clean_latents_and_conditions(
            original_image
        )
        
        self.log_memory_usage("After clean preparation")
        logger.info(f"Clean main latents shape: {clean_main_latents.shape}")
        logger.info(f"Subject condition latents shape: {subject_condition_latents.shape}")
        
        # ç¬¬äºŒæ­¥ï¼šè®¡ç®—cleançš„å»å™ªç»“æœ
        logger.info("Computing clean denoised latents...")
        with torch.no_grad():
            clean_denoised_latents = self.denoise_latents_with_perturbation(
                clean_main_latents, subject_condition_latents, pipeline_components, enable_grad=False
            )
            clean_denoised_latents = clean_denoised_latents.detach().to(device=str(self.device), dtype=torch.bfloat16)
            clean_denoised_latents.requires_grad_(False)
        
        self.log_memory_usage("After clean denoising")
        logger.info(f"Clean denoised latents shape: {clean_denoised_latents.shape}")
        
        # ç¬¬ä¸‰æ­¥ï¼šåˆå§‹åŒ–å¯å­¦ä¹ çš„æ‰°åŠ¨delta
        # æ‰°åŠ¨çš„shapeä¸main_latentsç›¸åŒ: (1, 4096, 16)
        delta_perturbation = torch.zeros_like(clean_main_latents, requires_grad=True, device=str(self.device), dtype=torch.bfloat16)
        
        # åˆå§‹åŒ–ä¸ºå°çš„éšæœºæ‰°åŠ¨
        with torch.no_grad():
            delta_perturbation.data = (torch.randn_like(clean_main_latents) * epsilon * 0.1).to(device=str(self.device), dtype=torch.bfloat16)
        
        attack_info = {
            'loss_history': [],
            'mse_history': [],
            'gradient_norms': [],
            'gradient_status': [],
            'perturbation_norms': [],
            'epsilon': epsilon,
            'alpha': alpha,
            'num_iterations': num_iterations,
            'lambda_reg': lambda_reg,
            'attack_prompt': self.attack_prompt,
            'space': 'direct_main_latents',
            'comparison': 'denoised_latents_mse'
        }
        
        logger.info(f"Starting DIRECT-LATENTS PGD attack with {num_iterations} iterations")
        logger.info(f"Attacking MAIN latents directly, shape: {clean_main_latents.shape}")
        logger.info(f"Delta epsilon: {epsilon}, alpha: {alpha}")
        
        consecutive_failures = 0
        max_consecutive_failures = 5
        
        for i in range(num_iterations):
            self.log_memory_usage(f"Iteration {i+1} start")
            
            delta_perturbation.requires_grad_(True)
            
            # ç¬¬å››æ­¥ï¼šåœ¨ä¸»è¦latentsä¸ŠåŠ æ‰°åŠ¨
            adversarial_main_latents = clean_main_latents + delta_perturbation
            
            # è®¡ç®—æ‰°åŠ¨å¹…åº¦
            perturbation_norm = delta_perturbation.norm().item()
            attack_info['perturbation_norms'].append(perturbation_norm)
            
            # ç¬¬äº”æ­¥ï¼šä½¿ç”¨æ‰°åŠ¨åçš„latentsè¿›è¡Œå»å™ª
            adversarial_denoised_latents = self.denoise_latents_with_perturbation(
                adversarial_main_latents, subject_condition_latents, pipeline_components, enable_grad=True
            )
            
            # ç¬¬å…­æ­¥ï¼šè®¡ç®—MSEæŸå¤±
            mse_loss = F.mse_loss(clean_denoised_latents, adversarial_denoised_latents)
            
            if not mse_loss.requires_grad:
                logger.warning(f"Iteration {i+1}: MSE has no gradient, attempting recovery...")
                consecutive_failures += 1
                
                if consecutive_failures >= max_consecutive_failures:
                    logger.error(f"Too many consecutive gradient failures ({consecutive_failures}), stopping attack")
                    break
                
                torch.cuda.empty_cache()
                continue
            
            consecutive_failures = 0  # é‡ç½®å¤±è´¥è®¡æ•°
            
            # æ­£åˆ™åŒ–é¡¹ï¼ˆL2èŒƒæ•°çº¦æŸï¼‰
            reg_loss = delta_perturbation.norm()
            
            # ç¬¬ä¸ƒæ­¥ï¼šæ€»æŸå¤±ï¼ˆæœ€å¤§åŒ–MSEï¼Œæœ€å°åŒ–æ­£åˆ™åŒ–ï¼‰
            total_loss = -mse_loss + lambda_reg * reg_loss
            
            # è®°å½•
            attack_info['loss_history'].append(total_loss.item())
            attack_info['mse_history'].append(mse_loss.item())
            
            logger.info(f"Iter {i+1}/{num_iterations}: MSE={mse_loss.item():.6f}, "
                       f"Reg={reg_loss.item():.6f}, Total={total_loss.item():.6f}, "
                       f"Delta_norm={perturbation_norm:.6f}")
            
            # ç¬¬å…«æ­¥ï¼šåå‘ä¼ æ’­å’Œæ¢¯åº¦æ›´æ–°
            total_loss.backward()
            
            if delta_perturbation.grad is None:
                logger.error(f"Iteration {i+1}: Gradient is None!")
                attack_info['gradient_status'].append('None')
                break
            
            # æ¢¯åº¦è¯Šæ–­
            grad_diagnosis = self.diagnose_gradient_flow(delta_perturbation, "delta_perturbation")
            grad_norm = delta_perturbation.grad.norm().item()
            attack_info['gradient_norms'].append(grad_norm)
            attack_info['gradient_status'].append('OK' if grad_diagnosis else 'Poor')
            
            # è‡ªé€‚åº”æ­¥é•¿è°ƒæ•´
            if grad_norm < 1e-8:
                alpha_adjusted = min(alpha * 10, epsilon)
            elif grad_norm > 1e3:
                alpha_adjusted = alpha * 0.1
            else:
                alpha_adjusted = alpha
            
            # PGDæ›´æ–°
            with torch.no_grad():
                # æ¢¯åº¦ä¸Šå‡ï¼ˆæœ€å¤§åŒ–MSEï¼‰
                delta_perturbation.data = delta_perturbation.data + alpha_adjusted * delta_perturbation.grad.sign()
                
                # Lâˆçº¦æŸæŠ•å½±
                delta_perturbation.data = torch.clamp(delta_perturbation.data, -epsilon, epsilon)
            
            # æ¸…é›¶æ¢¯åº¦
            delta_perturbation.grad = None
            
            # æ—©æœŸåœæ­¢
            if i > 10 and len(attack_info['mse_history']) > 10:
                recent_mse = attack_info['mse_history'][-10:]
                if max(recent_mse) - min(recent_mse) < 1e-6:
                    logger.warning(f"MSE converged early at iteration {i+1}")
                    break
            
            # å†…å­˜æ¸…ç†
            if i % 5 == 0:
                torch.cuda.empty_cache()
                self.log_memory_usage(f"Iteration {i+1} after cleanup")
        
        logger.info("DIRECT-LATENTS PGD attack completed!")
        logger.info(f"Final gradient status: {attack_info['gradient_status'][-5:] if attack_info['gradient_status'] else 'No gradients'}")
        logger.info(f"Final perturbation norm: {delta_perturbation.norm().item():.6f}")
        
        return delta_perturbation.detach(), attack_info
    
    def tensor_to_pil_official(self, tensor: torch.Tensor) -> Image.Image:
        """å°†tensorè½¬æ¢ä¸ºPILå›¾ç‰‡"""
        if len(tensor.shape) == 4:
            tensor = tensor.squeeze(0)
        
        tensor = tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
        image = self.pipe.image_processor.postprocess(tensor, output_type="pil")[0]
        return image
    
    def _test_basic_gradients(self):
        """æµ‹è¯•åŸºç¡€æ¢¯åº¦åŠŸèƒ½"""
        logger.info("Testing basic gradient functionality...")
        
        try:
            test_tensor = torch.randn(1, 4096, 16, device=str(self.device), requires_grad=True)  # main latents shape
            result = test_tensor * 2 + 1
            loss = result.mean()
            loss.backward()
            
            if test_tensor.grad is not None:
                grad_norm = test_tensor.grad.norm().item()
                logger.info(f"âœ“ Basic gradient test passed, grad norm: {grad_norm:.6f}")
                return True
            else:
                logger.error("âœ— Basic gradient test failed")
                return False
        except Exception as e:
            logger.error(f"âœ— Basic gradient test failed: {e}")
            return False
    
    def process_dataset(self, 
                       dataset,
                       epsilon: float = 0.1,  # latent space epsilon
                       alpha: float = 0.02,   # latent space alpha
                       num_iterations: int = 50,
                       lambda_reg: float = 0.1,
                       save_frequency: int = 100,
                       resume_from: Optional[int] = None):
        """å¤„ç†æ•°æ®é›†"""
        
        dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0, 
                              collate_fn=lambda batch: batch)
        
        total_samples = len(dataset)
        start_idx = resume_from or 0
        success_count = 0
        total_mse_improvement = 0.0
        results_log = []
        
        logger.info(f"Starting DIRECT-LATENTS adversarial generation for {total_samples} images")
        logger.info(f"Attack target: main latents shape (1, 4096, 16)")
        logger.info(f"MSE comparison: denoised latents space")
        logger.info(f"Parameters: epsilon={epsilon}, alpha={alpha}")
        
        with tqdm(dataloader, desc="Processing (DIRECT-LATENTS)") as pbar:
            for batch_idx, batch in enumerate(pbar):
                if batch_idx < start_idx:
                    continue
                
                try:
                    original_image = batch[0]['image']
                    image_path = batch[0]['image_path']
                    image_idx = batch[0]['index']
                    
                    # è·³è¿‡å°å›¾ç‰‡
                    if original_image.size[0] < 256 or original_image.size[1] < 256:
                        logger.warning(f"Skipping small image {image_path}")
                        continue
                    
                    self.log_memory_usage(f"Processing image {batch_idx+1}")
                    
                    # æ‰§è¡Œç›´æ¥latents PGDæ”»å‡»
                    delta_perturbation, attack_info = self.pgd_attack_direct_latents(
                        original_image=original_image,
                        epsilon=epsilon,
                        alpha=alpha,
                        num_iterations=num_iterations,
                        lambda_reg=lambda_reg
                    )
                    
                    # ç”Ÿæˆæœ€ç»ˆå¯¹æŠ—æ ·æœ¬
                    logger.info("Generating final adversarial sample...")
                    clean_main_latents, subject_condition_latents, pipeline_components = self.prepare_clean_latents_and_conditions(
                        original_image
                    )
                    
                    # åº”ç”¨æœ€ç»ˆæ‰°åŠ¨
                    final_adversarial_latents = clean_main_latents + delta_perturbation
                    
                    # æœ€ç»ˆå»å™ªå’ŒVAEè§£ç 
                    with torch.no_grad():
                        adversarial_denoised = self.denoise_latents_with_perturbation(
                            final_adversarial_latents, subject_condition_latents, pipeline_components, enable_grad=False
                        )
                        
                        # Unpackå’ŒVAEè§£ç 
                        adversarial_unpacked = self.pipe._unpack_latents(
                            adversarial_denoised, 
                            pipeline_components['height'], 
                            pipeline_components['width'], 
                            self.pipe.vae_scale_factor
                        )
                        adversarial_decoded = self.decode_latents_to_tensor(adversarial_unpacked)
                        adversarial_image = self.tensor_to_pil_official(adversarial_decoded)
                    
                    # ä¿å­˜ç»“æœ
                    clean_path = self.clean_dir / f"{image_idx:06d}_clean.png"
                    adversarial_path = self.adversarial_dir / f"{image_idx:06d}_adversarial.png"
                    
                    original_image.save(clean_path)
                    adversarial_image.save(adversarial_path)
                    
                    # è®°å½•ç»“æœ
                    final_mse = attack_info['mse_history'][-1] if attack_info['mse_history'] else 0
                    final_perturbation_norm = attack_info['perturbation_norms'][-1] if attack_info['perturbation_norms'] else 0
                    
                    result_entry = {
                        'image_idx': image_idx,
                        'original_path': str(image_path),
                        'clean_path': str(clean_path),
                        'adversarial_path': str(adversarial_path),
                        'final_mse': final_mse,
                        'final_perturbation_norm': final_perturbation_norm,
                        'attack_info': attack_info,
                        'timestamp': datetime.now().isoformat(),
                        'method': 'DIRECT_LATENTS_ATTACK',
                        'gradient_success': len([s for s in attack_info.get('gradient_status', []) if s == 'OK'])
                    }
                    results_log.append(result_entry)
                    
                    if final_mse > 0.01:
                        success_count += 1
                    total_mse_improvement += final_mse
                    
                    # è®¡ç®—æ¢¯åº¦æˆåŠŸç‡
                    gradient_success_rate = len([s for s in attack_info.get('gradient_status', []) if s == 'OK']) / max(len(attack_info.get('gradient_status', [])), 1)
                    
                    pbar.set_postfix({
                        'Success': f"{success_count}/{batch_idx+1}",
                        'Avg_MSE': f"{total_mse_improvement/(batch_idx+1):.4f}",
                        'Current_MSE': f"{final_mse:.4f}",
                        'Delta_norm': f"{final_perturbation_norm:.4f}",
                        'Grad_Success': f"{gradient_success_rate:.1%}",
                        'Method': 'DIRECT-LATENTS'
                    })
                    
                    # å®šæœŸä¿å­˜
                    if (batch_idx + 1) % save_frequency == 0:
                        log_path = self.logs_dir / f"direct_latents_progress_{batch_idx + 1}.json"
                        with open(log_path, 'w') as f:
                            json.dump(results_log, f, indent=2)
                
                except Exception as e:
                    logger.error(f"Failed to process image {batch_idx}: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
                
                # æ›´é¢‘ç¹çš„å†…å­˜æ¸…ç†
                if batch_idx % 2 == 0:
                    torch.cuda.empty_cache()
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        final_log_path = self.logs_dir / "direct_latents_final_results.json"
        
        # è®¡ç®—æ¢¯åº¦ç»Ÿè®¡
        total_gradient_checks = sum(len(r.get('attack_info', {}).get('gradient_status', [])) for r in results_log)
        successful_gradients = sum(len([s for s in r.get('attack_info', {}).get('gradient_status', []) if s == 'OK']) for r in results_log)
        gradient_success_rate = successful_gradients / max(total_gradient_checks, 1)
        
        summary = {
            'total_samples': total_samples,
            'success_count': success_count,
            'success_rate': success_count / total_samples if total_samples > 0 else 0,
            'average_mse': total_mse_improvement / max(success_count, 1),
            'gradient_statistics': {
                'total_gradient_checks': total_gradient_checks,
                'successful_gradients': successful_gradients,
                'gradient_success_rate': gradient_success_rate
            },
            'method_info': {
                'method': 'DIRECT_LATENTS_ATTACK',
                'attack_target': 'main_latents_shape_(1,4096,16)',
                'mse_comparison': 'denoised_latents_space',
                'advantages': [
                    'Direct perturbation on main latents',
                    'Stable gradient flow',
                    'Precise control over perturbation magnitude',
                    'Follows EasyControl pipeline exactly',
                    'Subject conditions remain fixed'
                ],
                'parameters': {
                    'epsilon': epsilon,
                    'alpha': alpha,
                    'num_iterations': num_iterations,
                    'lambda_reg': lambda_reg
                }
            },
            'results': results_log,
            'timestamp': datetime.now().isoformat()
        }
        
        with open(final_log_path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        logger.info(f"DIRECT-LATENTS generation completed!")
        logger.info(f"Success rate: {success_count}/{total_samples} ({success_count/total_samples*100:.1f}%)")
        logger.info(f"Gradient success rate: {gradient_success_rate:.1%}")
        logger.info(f"Average MSE: {total_mse_improvement/max(success_count, 1):.4f}")
 
def main():
    parser = argparse.ArgumentParser(description="DIRECT-LATENTS adversarial generation")
    
    # æ•°æ®é›†å‚æ•°
    parser.add_argument("--data_root", type=str, required=True,
                       help="Root directory of LAIONFace dataset")
    parser.add_argument("--subset_size", type=int, default=None,
                       help="Use only a subset of the dataset")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--base_model", type=str, default="/openbayes/input/input0",
                       help="Base FLUX model path")
    parser.add_argument("--subject_lora", type=str, default="/openbayes/input/input0/subject.safetensors",
                       help="Subject LoRA model path")
    
    # æ”»å‡»å‚æ•°
    parser.add_argument("--epsilon", type=float, default=0.1, 
                       help="Maximum perturbation magnitude in latent space")
    parser.add_argument("--alpha", type=float, default=0.02, 
                       help="PGD step size in latent space")
    parser.add_argument("--num_iterations", type=int, default=50,
                       help="Number of PGD iterations")
    parser.add_argument("--lambda_reg", type=float, default=0.1,
                       help="Regularization coefficient")
    
    # ç³»ç»Ÿå‚æ•°
    parser.add_argument("--output_dir", type=str, default="./direct_latents_adversarial_results",
                       help="Output directory")
    parser.add_argument("--device", type=str, default="cuda",
                       help="Computing device")
    parser.add_argument("--save_frequency", type=int, default=50,
                       help="Save progress every N samples")
    parser.add_argument("--resume_from", type=int, default=None,
                       help="Resume from specific sample index")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if args.device == "cuda" and not torch.cuda.is_available():
        logger.error("CUDA requested but not available! Falling back to CPU.")
        args.device = "cpu"
    
    # é¢å¤–çš„å†…å­˜ä¼˜åŒ–è®¾ç½®
    if torch.cuda.is_available():
        torch.backends.cuda.max_split_size_mb = 512
        logger.info("Set CUDA max split size to 512MB")
    
    # åˆ›å»ºæ•°æ®é›†
    logger.info(f"Loading dataset from {args.data_root}")
    dataset = LAIONFaceDataset(args.data_root, args.subset_size)
    
    # ä½¿ç”¨ç›´æ¥latentsæ”»å‡»ç”Ÿæˆå™¨
    logger.info("Initializing DIRECT-LATENTS adversarial generator")
    generator = DirectLatentsAdversarialGenerator(
        base_path=args.base_model,
        subject_lora_path=args.subject_lora,
        device=args.device,
        output_dir=args.output_dir
    )
    
    # å¼€å§‹å¤„ç†
    logger.info("Starting DIRECT-LATENTS adversarial generation")
    logger.info("ATTACK TARGET: main latents shape (1, 4096, 16)")
    logger.info("MSE comparison: denoised latents space")
    logger.info("METHOD: Direct delta perturbation on packed latents")
    logger.info(f"Parameters: epsilon={args.epsilon}, alpha={args.alpha}, iterations={args.num_iterations}")
    
    generator.process_dataset(
        dataset=dataset,
        epsilon=args.epsilon,
        alpha=args.alpha,
        num_iterations=args.num_iterations,
        lambda_reg=args.lambda_reg,
        save_frequency=args.save_frequency,
        resume_from=args.resume_from
    )
    
    logger.info("DIRECT-LATENTS generation completed!")
 
if __name__ == "__main__":
    main()
 
"""
ç›´æ¥åœ¨ä¸»è¦Latentsä¸ŠåŠ æ‰°åŠ¨çš„å¯¹æŠ—æ ·æœ¬ç”Ÿæˆå™¨

## ğŸ¯ ç®—æ³•æ ¸å¿ƒï¼š
1. è®°å½•clean_main_latentså’Œclean_denoised_latents  
2. åœ¨iterationä¸­ç›´æ¥å¯¹main_latentsåŠ å¯å­¦ä¹ çš„deltaæ‰°åŠ¨
3. åœ¨denoised_latentsæ¯”è¾ƒMSE
4. æ¢¯åº¦æ›´æ–°deltaå™ªå£°

## æ ¸å¿ƒä¼˜åŠ¿ï¼š
- âœ… ç›´æ¥åœ¨packed latents (1, 4096, 16)ä¸ŠåŠ æ‰°åŠ¨
- âœ… å®Œå…¨æŒ‰ç…§pipeline.pyçš„prepare_latentsé€»è¾‘
- âœ… ç¨³å®šçš„æ¢¯åº¦æµï¼ˆä¸ä¾èµ–éšæœºç§å­ï¼‰
- âœ… ç²¾ç¡®æ§åˆ¶æ‰°åŠ¨å¹…åº¦ï¼ˆLâˆçº¦æŸï¼‰
- âœ… Subjectæ¡ä»¶ä¿æŒå›ºå®šä¸å˜

## ä½¿ç”¨å‘½ä»¤ï¼š
python direct_latents_adversarial_generator.py \
    --data_root /openbayes/input/input0/sample_faces \
    --base_model /openbayes/input/input0 \
    --subject_lora /openbayes/input/input0/subject.safetensors \
    --epsilon 0.1 \
    --alpha 0.02 \
    --num_iterations 50 \
    --subset_size 1 \
    --device cuda

é¢„æœŸï¼šæ›´ç¨³å®šçš„æ¢¯åº¦ï¼Œæ›´ç›´æ¥çš„æ”»å‡»æ•ˆæœ
"""